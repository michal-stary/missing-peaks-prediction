{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "base = \"../data\"\n",
    "dataset = \"nist\"\n",
    "kind = \"in_database\"\n",
    "\n",
    "\n",
    "wv_path = f\"gas2vec/{kind}.model\"\n",
    "\n",
    "data_train_path =f\"{base}/{dataset}/{kind}/train.msp\"\n",
    "data_val_path =f\"{base}/{dataset}/{kind}/val.msp\"\n",
    "data_test_path =f\"{base}/{dataset}/{kind}/test.msp\"\n",
    "\n",
    "data_realtest_path = f\"{base}/3_RI using kovats of Mass spectra from RAMClustR on data 115.msp\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compounds to visualize \n",
    "if kind == \"in_database\":\n",
    "    interesting =  [\"Atrazine\", \"Carbofuran\", \"Monocrotophos\", \"Mevinphos\", \"Testosterone\"]\n",
    "elif kind == \"de_novo\":\n",
    "    interesting =  [\"Metobromuron\", \"Picoxystrobin\", \"Cyphenothrin\", \"Mirex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec.load(wv_path)\n",
    "vocab = {e:i for i, e in enumerate(w2v.wv.index2entity)}\n",
    "#vocab[\"unknown\"] = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchms.importing import load_from_msp\n",
    "from spec2vec import SpectrumDocument\n",
    "from utils.data_utils import spectrum_processing\n",
    "from utils.data_utils import IntegerMzCoder, TextMzCoder, TopKDS, GenDS, HuggDS\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_documents(path):\n",
    "    # Load data from MSP file and apply filters\n",
    "    spectrums = [spectrum_processing(s, min_rel_int=None, n_required_peaks=1) for s in load_from_msp(path, metadata_harmonization=False)]\n",
    "    # Omit spectrums that didn't qualify for analysis\n",
    "    spectrums = [s for s in spectrums if s is not None]\n",
    "    # Create spectrum documents\n",
    "    documents = [SpectrumDocument(s, n_decimals=0) for s in spectrums]\n",
    "    return documents, spectrums\n",
    "\n",
    "# documents_train = load_process_documents(data_train_path)\n",
    "# documents_val = load_process_documents(data_val_path)\n",
    "# documents_test = load_process_documents(data_test_path)\n",
    "documents_supertest, spectrums_supertest = load_process_documents(data_realtest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    # \"hugg_train\": HuggDS(documents_train, vocab, max_len=256), \n",
    "    # \"hugg_val\": HuggDS(documents_val, vocab, max_len=256), \n",
    "    # \"hugg_test\": HuggDS(documents_test, vocab, max_len=256),\n",
    "    \"hugg_supertest\": HuggDS(documents_supertest, vocab, max_len=256),\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    # \"train\": DataLoader(datasets[\"hugg_train\"], batch_size=64, shuffle=True, num_workers=8),\n",
    "    # \"val\": DataLoader(datasets[\"hugg_val\"], batch_size=256, shuffle=True, num_workers=8),\n",
    "    \"test\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "# Initializing a GPT2 configuration\n",
    "configuration = GPT2Config(n_positions=256, vocab_size=len(vocab)+1, \\\n",
    "                            bos_token_id=len(vocab), eos_token_id=len(vocab))#\\\n",
    "#                               pad_token_id=len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 1000,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 1000,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 256,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.4.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 1001\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a model from the configuration\n",
    "gpt2 = GPT2LMHeadModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = gpt2.config\n",
    "\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.w2v = w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, train_loader, val_loader, model, optimizer, loss_fn, \\\n",
    "                 lr_scheduler, device, style, model_name=\"model\"):\n",
    "        self.train_loader= train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.model = model \n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.model_name=model_name\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.style = style\n",
    "        \n",
    "        \n",
    "    def evaluate(self, loader):\n",
    "        eval_loss = 0\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        for b, batch in enumerate(loader):\n",
    "            with torch.no_grad():\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                if self.style == \"simple\":\n",
    "                    outputs = self.model(**batch, labels=batch[\"input_ids\"])\n",
    "                    loss = outputs.loss\n",
    "                \n",
    "                eval_loss += loss.item()\n",
    "            \n",
    "        eval_loss /= len(loader)\n",
    "        \n",
    "        return eval_loss\n",
    "    \n",
    "    def train(self,n_epochs, early_stopping=True, patience=3, verbose=False):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        if early_stopping:\n",
    "            min_val_loss = np.inf\n",
    "            no_improve = 0\n",
    "        \n",
    "        \n",
    "        e_bar = tqdm.tqdm(range(n_epochs), total=n_epochs, leave=True, desc=\"Epoch: \")\n",
    "        for e in e_bar:\n",
    "            # train\n",
    "            train_loss = 0\n",
    "            self.model.train()\n",
    "            for b, batch in enumerate(self.train_loader):\n",
    "                \n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                if self.style == \"simple\":\n",
    "                    outputs = self.model(**batch, labels=batch[\"input_ids\"])\n",
    "                    loss = outputs.loss\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "                if self.lr_scheduler is not None:\n",
    "                    self.lr_scheduler.step()\n",
    "\n",
    "                if verbose and b % (len(self.train_loader)//100) == 0:\n",
    "                    print(f\"Done: {b}/{len(self.train_loader)}\")\n",
    "            \n",
    "            # evaluate (train, validation)\n",
    "            train_loss = self.evaluate(self.train_loader)\n",
    "            val_loss = self.evaluate(self.val_loader)\n",
    "\n",
    "            # early stopping\n",
    "            if early_stopping:\n",
    "                if val_loss < min_val_loss:\n",
    "                    min_val_loss = val_loss\n",
    "                    torch.save(self.model, f\"models/{kind}/{self.model_name}\")\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "            if no_improve > patience:\n",
    "                print(\"Early stopped\")\n",
    "                self.model = torch.load(f\"models/{kind}/{self.model_name}\")\n",
    "                break\n",
    "            \n",
    "            #print(f\"After {e+1} epochs: \")\n",
    "            e_bar.set_postfix({'Train loss': f\"{train_loss:.3}\",'Val loss': f\"{val_loss:.3}\" })\n",
    "            #print(f\"Train loss: {train_loss:.3}\")\n",
    "            #print(f\"Val loss: {val_loss:.3}\\n\")\n",
    "                \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            np.save(f\"losses/{kind}/{self.model_name}_train_loss.npy\", self.train_losses)\n",
    "            np.save(f\"losses/{kind}/{self.model_name}_val_loss.npy\", self.val_losses)\n",
    "\n",
    "        np.save(f\"losses/{kind}/{self.model_name}_train_loss.npy\", self.train_losses)\n",
    "        np.save(f\"losses/{kind}/{self.model_name}_val_loss.npy\", self.val_losses)\n",
    "        \n",
    "        return self.train_losses, self.val_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# discver whether to use GPU or not\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = gpt2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "optimizer = AdamW(gpt2.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "num_training_steps = num_epochs * len(dataloaders[\"train\"])\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in gpt2.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Trainable params: {pytorch_total_params//(10**6)}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_learner = Learner(dataloaders[\"train\"], dataloaders[\"val\"], gpt2, optimizer, None, \\\n",
    "                      None, device, style=\"simple\", model_name=\"gpt2_lr5e-5_ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = gpt2_learner.train(num_epochs, early_stopping=True, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PredictorDstGPT2():\n",
    "    def __init__(self, model, coder_class, max_mz=None):\n",
    "        self.model = model\n",
    "        self.coder = coder_class(model.w2v, max_mz)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, file, coder_cls, max_mz=None, device=\"cpu\"):\n",
    "        model = torch.load(file,  map_location=torch.device(device)) \n",
    "        return cls(model, coder_class=coder_cls, max_mz=max_mz)\n",
    "    \n",
    "    \n",
    "    def __call__(self, X_ds, device=\"cpu\"):\n",
    "        self.model= self.model.to(device)\n",
    "        X_ds = X_ds.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # return distribution for next peak \n",
    "            return F.softmax(self.model(**{\"input_ids\":X_ds}).logits[:, len(X_ds[0])-1],  dim=-1)\n",
    "    \n",
    "    def predict_l_next(self, X_dict_batch, l, filtered=True, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        receives indices and intens and predicts l next peaks\n",
    "        if filtered: returns only non-repetitive indices that are not given as input\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**X_dict_batch).logits[:, len(X_dict_batch[\"input_ids\"][0])-1]\n",
    "        \n",
    "        if filtered:\n",
    "            args_batch = torch.argsort(logits, dim=-1, descending=True).int().cpu()\n",
    "            res = torch.empty((len(X_dict_batch[\"input_ids\"]), l), dtype=torch.int)\n",
    "            for i in range(len(args_batch)):\n",
    "                given = set(X_dict_batch[\"input_ids\"][i])\n",
    "                solved = 0\n",
    "                for j in range(len(args_batch[0])):\n",
    "                    if args_batch[i][j] not in given:\n",
    "                        res[i][solved] = args_batch[i][j]\n",
    "                        solved +=1\n",
    "                    if solved == l:\n",
    "                        break\n",
    "                \n",
    "            return res.numpy()\n",
    "        \n",
    "        return torch.argsort(logits, dim=-1, descending=True)[:, :l].cpu().numpy()\n",
    "        \n",
    "\n",
    "    def predict_l_all(self, hugg_ds, up_to_k, l, batch_size=64, filtered=True, verbose=False, device=\"cpu\"):\n",
    "        \n",
    "        self.model= self.model.to(device)\n",
    "        \n",
    "        #loader = DataLoader(hugg_ds, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "        def loader_gen(ds, batch_size):\n",
    "            keys= ds[0].keys()\n",
    "            for i in range((len(ds)//batch_size)):\n",
    "                n_sam = batch_size #if i*batch_size < len(ds) else len(ds) % batch_size\n",
    "                batch = {k: torch.empty(n_sam, *v.shape, dtype=v.dtype) for k,v in ds[0].items()}\n",
    "                for j in range(n_sam):\n",
    "                    for key in keys:\n",
    "                        batch[key][j] = ds[i*batch_size+j][key]\n",
    "                yield batch\n",
    "            \n",
    "            n_sam = len(ds)% batch_size\n",
    "            \n",
    "            batch = {k: torch.empty(n_sam, *v.shape, dtype=v.dtype) for k,v in ds[0].items()}\n",
    "            for j in range(n_sam):\n",
    "                for key in keys:\n",
    "                    batch[key][j] = ds[(i+1)*batch_size+j][key]\n",
    "            yield batch\n",
    "        \n",
    "        loader = loader_gen(hugg_ds, batch_size)\n",
    "        len_loader = len(hugg_ds)//batch_size + 1\n",
    "\n",
    "        X_intens = [np.sort(doc.peaks.intensities)[::-1][:min(self.model.config.n_positions, len(doc.peaks.intensities))-1] for doc in hugg_ds.ref_docs]\n",
    "        \n",
    "        l_pred_indices_per_k = np.ones(shape=(up_to_k, len(hugg_ds), l)) * (-1)\n",
    "        y_indices = []\n",
    "        for b, X_dict_batch_ in enumerate(loader):\n",
    "                \n",
    "#             indices, intens = X_ds[:,0].numpy().astype(int), X_ds[:,1].numpy()\n",
    "            y_indices += [arr[(X_dict_batch_[\"attention_mask\"][u]==1).numpy()][1:] for u, arr in enumerate((X_dict_batch_[\"input_ids\"].numpy().astype(int)))]            \n",
    "    #             X_intens.append(intens)\n",
    "            for j in range(1, up_to_k+1):\n",
    "                # we skip evaluation for too short spectra\n",
    "#                 if j > len(indices):\n",
    "#                     continue\n",
    "               # print(j)\n",
    "                X_dict_batch = {\"input_ids\": X_dict_batch_[\"input_ids\"][:, :j].clone()}#{k: v.detach().clone() for k, v in X_dict_batch_.items()}\n",
    "                \n",
    "               # drop too short spectra\n",
    "                # get mask - for each sample in batch either True (long enough) or False (too short) \n",
    "                len_mask = X_dict_batch_[\"attention_mask\"][:, j-1] == 1\n",
    "                \n",
    "                X_dict_batch[\"input_ids\"] = X_dict_batch[\"input_ids\"][len_mask]\n",
    "\n",
    "                # no samples in batch left after cutting \n",
    "                if len_mask.sum() == 0:\n",
    "                    continue\n",
    "                X_dict_batch = {k: v.to(device) for k, v in X_dict_batch.items()}\n",
    "\n",
    "                l_next = self.predict_l_next(X_dict_batch, l, filtered, device=device)\n",
    "                \n",
    "                l_pred_indices_per_k[j-1,b*batch_size: (b+1)*(batch_size)][len_mask] = l_next\n",
    "            \n",
    "            if verbose and b % 1 == 0:\n",
    "                print(f\"Done: {b}/{len_loader}\")\n",
    "        return l_pred_indices_per_k, y_indices, X_intens \n",
    "\n",
    "\n",
    "class PredictorAutoGPT2():\n",
    "    def __init__(self, model, coder_class, max_mz=None):\n",
    "        self.model = model\n",
    "        self.coder = coder_class(model.w2v, max_mz)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, file, coder_cls, max_mz=None, device=\"cpu\"):\n",
    "        model = torch.load(file,  map_location=torch.device(device)) \n",
    "        return cls(model, coder_class=coder_cls, max_mz=max_mz)\n",
    "    \n",
    "    \n",
    "    def __call__(self, X_ds, device=\"cpu\"):\n",
    "        self.model= self.model.to(device)\n",
    "        X_ds = X_ds.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # return distribution for next peak \n",
    "            return F.softmax(self.model(**{\"input_ids\":X_ds}).logits[:, len(X_ds[0])-1], dim=-1)\n",
    "    \n",
    "    def predict_l_next(self, X_dict_batch, l, filtered=True, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        receives indices and intens and predicts l next peaks\n",
    "        if filtered: returns only non-repetitive indices that are not given as input\n",
    "        \"\"\"\n",
    "        \n",
    "        k = X_dict_batch[\"input_ids\"].shape[1]\n",
    "\n",
    "        if filtered:\n",
    "            return self.model.generate(\n",
    "                X_dict_batch[\"input_ids\"], \n",
    "                max_length=k+l, \n",
    "                #num_beams=5, \n",
    "                no_repeat_ngram_size=1, \n",
    "                #encoder_no_repeat_ngram_size=1,\n",
    "                #early_stopping=True, \n",
    "                #bad_words_ids = X_dict_batch[\"input_ids\"].tolist(),\n",
    "                pad_token_id=len(self.coder.vocab)\n",
    "            ).cpu().numpy()[:, k:]\n",
    "\n",
    "#             res = torch.empty(len(X_dict_batch[\"input_ids\"]), l, dtype=torch.int) \n",
    "#             for i in range(len(beam_output)):\n",
    "#                 uniq_row = torch.unique(beam_output[i])\n",
    "#                 assert len(uniq_row) >= l \n",
    "#                 res[i, :] = uniq_row[:l]\n",
    "#             return res\n",
    "            \n",
    "        \n",
    "        return self.model.generate(\n",
    "                X_dict_batch[\"input_ids\"], \n",
    "                max_length=k+l, \n",
    "                #num_beams=5, \n",
    "                #early_stopping=True, \n",
    "                pad_token_id=len(self.coder.vocab)\n",
    "        ).cpu().numpy()[:, k:]\n",
    "        \n",
    "#       \n",
    "    def predict_l_all(self, hugg_ds, up_to_k, l, batch_size=64, filtered=True, verbose=False, device=\"cpu\"):\n",
    "        self.model= self.model.to(device)\n",
    "        \n",
    "        #loader = DataLoader(hugg_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        def loader_gen(ds, batch_size):\n",
    "            keys= ds[0].keys()\n",
    "            for i in range((len(ds)//batch_size)):\n",
    "                n_sam = batch_size #if i*batch_size < len(ds) else len(ds) % batch_size\n",
    "                batch = {k: torch.empty(n_sam, *v.shape, dtype=v.dtype) for k,v in ds[0].items()}\n",
    "                for j in range(n_sam):\n",
    "                    for key in keys:\n",
    "                        batch[key][j] = ds[i*batch_size+j][key]\n",
    "                yield batch\n",
    "            \n",
    "            n_sam = len(ds)% batch_size\n",
    "            \n",
    "            batch = {k: torch.empty(n_sam, *v.shape, dtype=v.dtype) for k,v in ds[0].items()}\n",
    "            for j in range(n_sam):\n",
    "                for key in keys:\n",
    "                    batch[key][j] = ds[(i+1)*batch_size+j][key]\n",
    "            yield batch\n",
    "            \n",
    "        loader = loader_gen(hugg_ds, batch_size)\n",
    "        len_loader = len(hugg_ds)//batch_size + 1\n",
    "        X_intens = [np.sort(doc.peaks.intensities)[::-1][:min(self.model.config.n_positions, len(doc.peaks.intensities))-1] for doc in hugg_ds.ref_docs]\n",
    "\n",
    "        \n",
    "        l_pred_indices_per_k = np.ones(shape=(up_to_k, len(hugg_ds), l)) * (-1)\n",
    "        y_indices = []\n",
    "\n",
    "        for b, X_dict_batch_ in enumerate(loader):\n",
    "                \n",
    "#             indices, intens = X_ds[:,0].numpy().astype(int), X_ds[:,1].numpy()\n",
    "            y_indices += [arr[(X_dict_batch_[\"attention_mask\"][u]==1).numpy()][1:] for u, arr in enumerate((X_dict_batch_[\"input_ids\"].numpy().astype(int)))]\n",
    "#             X_intens.append(intens)\n",
    "            for j in range(1, up_to_k+1):\n",
    "                # we skip evaluation for too short spectra\n",
    "#                 if j > len(indices):\n",
    "#                     continue\n",
    "               # print(j)\n",
    "                X_dict_batch = {\"input_ids\": X_dict_batch_[\"input_ids\"][:, :j].clone()}#{k: v.detach().clone() for k, v in X_dict_batch_.items()}\n",
    "                \n",
    "                # drop too short spectra\n",
    "                # get mask - for each sample in batch either True (long enough) or False (too short) \n",
    "                len_mask = X_dict_batch_[\"attention_mask\"][:, j-1] == 1\n",
    "                \n",
    "                X_dict_batch[\"input_ids\"] = X_dict_batch[\"input_ids\"][len_mask]\n",
    "\n",
    "                # no samples in batch left after cutting \n",
    "                if len_mask.sum() == 0:\n",
    "                    continue\n",
    "                X_dict_batch = {k: v.to(device) for k, v in X_dict_batch.items()}\n",
    "\n",
    "                l_next = self.predict_l_next(X_dict_batch, l, filtered, device=device)\n",
    "                \n",
    "                l_pred_indices_per_k[j-1,b*batch_size: (b+1)*(batch_size)][len_mask] = l_next\n",
    "            \n",
    "            if verbose and b % 1 == 0:\n",
    "                print(f\"Done: {b}/{len_loader}\")\n",
    "        return l_pred_indices_per_k, y_indices, X_intens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visual_utils import plot_spectrum_predictions\n",
    "\n",
    "predictor = PredictorDstGPT2.from_file(f\"models/{kind}/gpt2_lr5e-5\", TextMzCoder, max_mz=None)\n",
    "predictor.model.cpu()\n",
    "i = 100\n",
    "k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ds = TopKDS(documents_val, vocab, k, onehot=False, add_intensity=False)\n",
    "prob = predictor(plot_ds[i][0].reshape(1, *plot_ds[i][0].shape))[0,:-1].cpu().numpy()\n",
    "plot_spectrum_predictions(plot_ds.ref_docs[i], k, prob, predictor.coder, plot_full=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "done on the val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.metrics import metrics_klj, metrics_intlj\n",
    "\n",
    "predictors = { \n",
    "            \"gpt2_first_dst\": PredictorDstGPT2.from_file(f\"models/{kind}/gpt2\", TextMzCoder, max_mz=None, device=device),\n",
    "            \"gpt2_ns_dst\": PredictorDstGPT2.from_file(f\"models/{kind}/gpt2_lr5e-5_ns\", TextMzCoder, max_mz=None, device=device),  \n",
    "            \"gpt2_dst\": PredictorDstGPT2.from_file(f\"models/{kind}/gpt2_lr5e-5\", TextMzCoder, max_mz=None, device=device),  \n",
    "            \"gpt2_first_auto\": PredictorAutoGPT2.from_file(f\"models/{kind}/gpt2\", TextMzCoder, max_mz=None, device=device),\n",
    "            \"gpt2_auto\": PredictorAutoGPT2.from_file(f\"models/{kind}/gpt2_lr5e-5\", TextMzCoder, max_mz=None, device=device),\n",
    "            \"gpt2_ns_auto\": PredictorAutoGPT2.from_file(f\"models/{kind}/gpt2_lr5e-5_ns\", TextMzCoder, max_mz=None, device=device),\n",
    "                \n",
    "             }\n",
    "\n",
    "P_FOLDER = f\"predictions/{kind}\"\n",
    "up_to_k = 30\n",
    "l = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_evaluation import calc_predictions, load_predictions, model_selection\n",
    "from utils.visual_utils import plot_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "calc_predictions(up_to_k, l, predictors, datasets, \"hugg_val\", P_FOLDER, batch_size=512, device=device, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p_name, scores = model_selection(P_FOLDER, \"hugg_val\", predictors=predictors, up_to_k=20, \\\n",
    "                                      l=4, j=6, to_rel_inten=.2, l_rel=None)\n",
    "plot_scores(scores, metrics=\"mpi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_f = lambda x: [\"ns\",\"first\"].index(x.split(\"_\")[1]) \\\n",
    "if len(x.split(\"_\")) > 2 else 3\n",
    "\n",
    "orderer_f = lambda x: str(hue_f(x)) + x\n",
    "\n",
    "plot_scores(scores, metrics=\"mpi\", grouper_f=lambda x: x.split(\"_\")[-1],\n",
    "            orderer_f=orderer_f,hue_f=hue_f,\n",
    "            title= \"De novo\" if kind==\"de_novo\" else \"In database\",\n",
    "            xlabel= \"k\", ylabel=\"meanPrecision\",\n",
    "            save_to_path=f\"figures/{kind}/selection/gpt_val_prec.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p_name = \"gpt2_ns_auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of the best predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "i = 14047\n",
    "plot_ds = TopKDS(documents_test, vocab, k, onehot=False, add_intensity=False)\n",
    "prob = predictors[best_p_name](plot_ds[i][0].reshape(1, *plot_ds[i][0].shape))[0,:-1].cpu().numpy()\n",
    "plot_spectrum_predictions(plot_ds.ref_docs[i], k, prob, \\\n",
    "                         predictors[best_p_name].coder,\\\n",
    "                         plot_full=False, \\\n",
    "                         save_to_path=f\"figures/{kind}/visual/gpt_test_topk.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(documents_test)):\n",
    "    if documents_test[i].metadata[\"name\"] not in interesting:\n",
    "        continue\n",
    "    print(documents_test[i].metadata[\"name\"])\n",
    "    for k in [3,4,5,6,7,8]:\n",
    "        plot_ds = TopKDS(documents_test, vocab, k, onehot=False, add_intensity=False)\n",
    "        prob = predictors[best_p_name](plot_ds[i][0].reshape(1, *plot_ds[i][0].shape))[0,:-1].cpu().numpy()\n",
    "        plot_spectrum_predictions(plot_ds.ref_docs[i], k, prob, \\\n",
    "                                 predictors[best_p_name].coder,\\\n",
    "                                 plot_full=False, \\\n",
    "                                 save_to_path=f\"figures/{kind}/visual/gpt_test_{i}_top{k}.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed evaluation of the best predictor \n",
    "done on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "up_to_k = 30\n",
    "l = 20\n",
    "calc_predictions(up_to_k, l,  {best_p_name: predictors[best_p_name]}, datasets, \\\n",
    "                 \"hugg_test\", P_FOLDER, batch_size=512, \\\n",
    "                 device=device, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_pred_indices_per_k_best, y_indices, X_intens = load_predictions(best_p_name, \"hugg_test\", P_FOLDER)\n",
    "l_pred_indices_per_k_base, y_indices_base, X_intens_base = load_predictions(\"specknn_1\", \"spec_test\", P_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import accuracy_at_k\n",
    "from utils.visual_utils import plot_stats\n",
    "\n",
    "def get_to_len(xs, len_):\n",
    "    return [xs[i] if i<len(xs) else 0 for i in range(len_)]\n",
    "\n",
    "accs_at_k = accuracy_at_k(l_pred_indices_per_k_best, y_indices, up_to_k=None)\n",
    "accs_at_k_base = get_to_len(accuracy_at_k(l_pred_indices_per_k_base, y_indices_base, up_to_k=None), len(accs_at_k))\n",
    "\n",
    "\n",
    "\n",
    "plot_stats(accs_at_k, baseline1D=accs_at_k_base, title=\"Accuracy@k\", color=\"red\", ylabel='accuracy', xlabel='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import accuracy_at_int\n",
    "\n",
    "accs_at_int = accuracy_at_int(l_pred_indices_per_k_best, y_indices, X_intens)\n",
    "accs_at_int_base = get_to_len(accuracy_at_int(l_pred_indices_per_k_base, y_indices_base, X_intens_base), len(accs_at_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(accs_at_int, baseline1D=accs_at_int_base, title=\"Accuracy@int\", color=\"red\", decreasing=True,  ylabel='accuracy', xlabel='intensity level', x_factor=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p_name, scores_test = model_selection(P_FOLDER, \"hugg_test\", \\\n",
    "                                           predictors={best_p_name: predictors[best_p_name]}, up_to_k=30, \\\n",
    "                                           l=4, j=6, to_rel_inten=.2, l_rel=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_scores(scores_test, metrics=\"mpi\", grouper_f=lambda x: x.split(\"_\")[-1],\n",
    "            orderer_f=orderer_f,hue_f=hue_f,\n",
    "            title= \"De novo\" if kind==\"de_novo\" else \"In database\",\n",
    "            xlabel= \"k\", ylabel=\"meanPrecision\",\n",
    "            save_to_path=f\"figures/{kind}/selection/gpt_test_prec.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supertest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugg_ds = datasets[\"hugg_supertest\"]\n",
    "batch_size = 1\n",
    "predictor = predictors[best_p_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data_utils.HuggDS at 0x7fb483f65470>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loader_gen(ds, batch_size):\n",
    "    keys= ds[0].keys()\n",
    "    for i in range((len(ds)//batch_size)):\n",
    "        n_sam = batch_size #if i*batch_size < len(ds) else len(ds) % batch_size\n",
    "        batch = {k: torch.empty(n_sam, *v.shape, dtype=v.dtype) for k,v in ds[0].items()}\n",
    "        for j in range(n_sam):\n",
    "            for key in keys:\n",
    "                batch[key][j] = ds[i*batch_size+j][key]\n",
    "        yield batch\n",
    "            \n",
    "    n_sam = len(ds)% batch_size\n",
    "            \n",
    "    batch = {k: torch.empty(n_sam, *v.shape, dtype=v.dtype) for k,v in ds[0].items()}\n",
    "    for j in range((len(ds)%batch_size)):\n",
    "        for key in keys:\n",
    "            batch[key][j] = ds[(i+1)*batch_size+j][key]\n",
    "        yield batch\n",
    "            \n",
    "loader = loader_gen(hugg_ds, batch_size)\n",
    "len_loader = len(hugg_ds)//batch_size + 1\n",
    "X_intens = [np.sort(doc.peaks.intensities)[::-1][:min(predictor.model.config.n_positions, len(doc.peaks.intensities))-1] for doc in hugg_ds.ref_docs]\n",
    "\n",
    "datasets[\"hugg_supertest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.coder.vocab[\"peak@115\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hugg_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [28:21, 25.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preds = []\n",
    "for b, X_dict_batch_ in enumerate(tqdm(loader)):\n",
    "    #print(X_dict_batch_)\n",
    "    X_dict_batch = {\"input_ids\": torch.atleast_2d(X_dict_batch_[\"input_ids\"][X_dict_batch_[\"attention_mask\"] == 1])}\n",
    "    #print(X_dict_batch)\n",
    "    pred = predictor.predict_l_next(X_dict_batch, l=10)[0]\n",
    "    pred = predictor.coder.index2mz[pred]\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spec2vec.SpectrumDocument.SpectrumDocument at 0x7fb3ef8d78d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugg_ds.ref_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ionmode': 'Negative',\n",
       " 'spectrumtype': 'Centroid',\n",
       " 'num_peaks': '19',\n",
       " 'compound_name': 'C11',\n",
       " 'retention_time': '122.61',\n",
       " 'retention_index': '1085.917874'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrums_supertest[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 72.05694212,  73.06470108,  76.0181442 ,  78.01498857,\n",
       "        80.03861506,  86.06807381,  86.07251984, 100.08824961,\n",
       "       114.09945172, 120.08875599, 127.11156231, 176.06747288,\n",
       "       302.12101639])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrums_supertest[0].peaks.mz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.prediction_utils import enhance_spectra, predict_spectra\n",
    "        \n",
    "enhanced_spectra = [*enhance_spectra(spectrums_supertest, preds, 5)]\n",
    "predicted_spectra = [*predict_spectra(spectrums_supertest, preds, 5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchms.exporting import save_as_msp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_msp(enhanced_spectra, f\"{data_realtest_path[:-4]}_enh_GPT2_5.msp\")\n",
    "save_as_msp(predicted_spectra, f\"{data_realtest_path[:-4]}_pred_GPT2_5.msp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "put(f\"{data_realtest_path[:-4]}_enh_GPT2_5.msp\")\n",
    "put(f\"{data_realtest_path[:-4]}_pred_GPT2_5.msp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
